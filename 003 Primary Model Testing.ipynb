{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Primary Model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPAUIfeMgqdwiEhD4nXWlnL"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"WQmXvERQ9Gwd"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torch.optim as optim                                                     # For gradient descent\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Creating a CNN\n","class SignClassifier(nn.Module):\n","\n","    def __init__(self):\n","        super(SignClassifier, self).__init__()\n","        self.name = \"Net\"                                                       \n","        self.conv1 = nn.Conv2d(3,5,5)                                           # First kernel is a 5 by 5, 3 color channels, it has output of 5    -> given 32x32x3, you are left with 28x28x5\n","        self.pool = nn.MaxPool2d(2,2)                                           # Max pooling layer with kernel size 2 and stride 2                 -> you are left with 14x14x5\n","        self.conv2 = nn.Conv2d(5,10,3)                                          # Second kernel is 5 by 5, it changes input depth from 5 to 10      -> you are left with 10x10x10\n","        self.conv3 = nn.Conv2d(10,12,5)                                         # Third kernel is 5 by 5, it changes input depth from 10 to 12      -> you are left with 6x6x12\n","                                                                                \n","        self.fc1 = nn.Linear(8*8*12, 200)                                       # Fully Connected Layers\n","        self.fc2 = nn.Linear(200, 100)\n","        self.fc3 = nn.Linear(100,9)                                             # 9 possible outputs\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))                                    # Apply first kernel, then activation function, then max pooling \n","        x = F.relu(self.conv2(x))                                               # Apply second kernel, then activation function\n","        x = F.relu(self.conv3(x))                                               # Apply second kernel, then activation function\n","        x = x.view(-1, 8*8*12)                                                  # flatten tensor for ANN portion\n","        x = F.relu(self.fc1(x))                                                 # Apply activation function on first fully connected layer\n","        x = F.relu(self.fc2(x))                                                 # Apply activation function on second fully connected layer\n","        x = self.fc3(x)                                                         # final activation function is included with criterion\n","        return x\n"],"execution_count":null,"outputs":[]}]}